{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch import Generator\n",
    "\n",
    "from transformers import CLIPProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset image sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets\n",
    "\n",
    "- Real faces: `ffhq_real_faces`\n",
    "    - 3143 images\n",
    "    - these are all in `png` format\n",
    "- Diffusion-generated faces (set 1): `AIS-4SD/StableDiffusion-3-faces-20250203-1545`\n",
    "    - 500 images\n",
    "    - these are all in `png` format\n",
    "- Diffusion-generated faces (set 2): `SFHQ-T2I`\n",
    "    - 1724 images\n",
    "    - these are all in `jpg` format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of these datasets, we are interested in the image size and whether it's consistent for all images within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_sizes(image_folder_path: Path) -> list:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    image_names = os.listdir(image_folder_path)\n",
    "    image_sizes = []\n",
    "    for i in tqdm(range(len(image_names))):\n",
    "        test_image_path = image_folder_path / image_names[i]\n",
    "        test_img = Image.open(test_image_path)\n",
    "        image_sizes.append(test_img.size)\n",
    "    return image_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_images_path = Path(\"data/ffhq_real_faces\")\n",
    "image_sizes = get_image_sizes(real_images_path)\n",
    "Counter(image_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images in this dataset have size (1024, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIS-4SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_images_1_path = Path(\"data/AIS-4SD/StableDiffusion-3-faces-20250203-1545\")\n",
    "image_sizes = get_image_sizes(synth_images_1_path)\n",
    "Counter(image_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images in this dataset have size (768, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFHQ-T2I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_images_2_path = Path(\"data/SFHQ-T2I\")\n",
    "image_sizes = get_image_sizes(synth_images_2_path)\n",
    "Counter(image_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images in this dataset have size (1024, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All 3143 real images have size (1024, 1024)\n",
    "- 1724 of the diffusion-generated images have size (1024, 1024), but 500 of them have size (768, 768)\n",
    "\n",
    "I could upscale the smaller images, but it would be safer (less likely to introduce image artifacts) to reduce the size of the larger images to (768, 768)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will experiment image pre-processing techniques in this notebook as it will be easier to display the images and understand how they are transformed by various functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recap of PyTorch dataset functionality:\n",
    "- `torch.utils.data.Dataset` stores the samples and their corresponding labels\n",
    "    - Each time it's called, it returns an [input, label] pair\n",
    "    - Pre-processing functions can be defined / called inside this class\n",
    "    - A custom Dataset class must implement three functions: __init__, __len__, and __getitem__\n",
    "- `torch.utils.data.DataLoader` wraps an iterable around the Dataset to enable easy access to the samples   \n",
    "    - Enabled iteration through the dataset in batches\n",
    "    - Provides access to in-built functions for shuffling, parallel processing etc\n",
    "    - Calls the `__getitem__()` function from the Dataset class to create a batch of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {\n",
    "    \"real\": 0,\n",
    "    \"synthetic\": 1\n",
    "}\n",
    "\n",
    "\n",
    "image_folder_names_to_labels = {\n",
    "    \"ffhq_real_faces\": 0,\n",
    "    \"AIS-4SD/StableDiffusion-3-faces-20250203-1545\": 1,\n",
    "    \"SFHQ-T2I\": 1\n",
    "}\n",
    "\n",
    "data_root_dir = Path(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll define the image augmentations.\n",
    "\n",
    "We already identified a difference in the original sizes of the images, and decided to resize all images to 768 by 768 pixels.\n",
    "\n",
    "We also want to ...\n",
    "\n",
    "We then need to convert the images to tensors as this is the format required by PyTorch models. This conversion also scales the pixel values to between 0-1.\n",
    "\n",
    "Finally, we need to normalise the pixel values in the same way that the training images were normalised. Since we are fine-tuning a pre-trained CLIP model, we can find the mean and standard deviation values used to normalise the CLIP training images by importing `CLIPProcessor` from the `transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "clip_mean = clip_processor.image_processor.image_mean\n",
    "clip_std = clip_processor.image_processor.image_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_size = (768, 768)\n",
    "\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize(size=target_image_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=clip_mean,\n",
    "        std=clip_std\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceImageDataset:\n",
    "    def __init__(self, data_root_dir: Path, transform, labels_dict: dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_root_dir: Path to directory containing image subdirectories\n",
    "        \"\"\"\n",
    "        self.data_root_dir = data_root_dir\n",
    "        self.samples = []\n",
    "        for class_label_int in list(labels_dict.values()):\n",
    "            self.get_images_and_labels(class_label_int)\n",
    "        self.transform = transform\n",
    "\n",
    "    def get_images_and_labels(self, class_label_int: int):\n",
    "        folder_paths = [folder_path for folder_path, class_label in image_folder_names_to_labels.items() if class_label==class_label_int]\n",
    "        for folder_path in folder_paths:\n",
    "            for image_name in os.listdir(self.data_root_dir / folder_path):\n",
    "                if image_name.lower().endswith((\".png\", \".jpg\")):\n",
    "                    image_path = self.data_root_dir / folder_path / image_name\n",
    "                    self.samples.append((image_path, class_label_int))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples\"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Get one sample\n",
    "        Returns:\n",
    "            image: Transformed image tensor\n",
    "            label: 0 for real, 1 for synthetic\n",
    "        \"\"\"\n",
    "        image_path, label = self.samples[idx]\n",
    "        image = Image.open(image_path)\n",
    "        transformed_image_tensor = self.transform(image)\n",
    "        assert transformed_image_tensor.shape[0] == 3, \"Unexpected number of channels; expected 3 for RGB.\"\n",
    "        return transformed_image_tensor, label\n",
    "        \n",
    "\n",
    "full_dataset = FaceImageDataset(data_root_dir, image_transforms, labels_dict)\n",
    "len(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start with a train/val/test split of 0.6/0.2/0.2 so that we have just over 1000 images in the validation and test sets\n",
    "\n",
    "train_size = int(0.6 * len(full_dataset))\n",
    "val_test_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_test_dataset = random_split(\n",
    "    full_dataset, \n",
    "    [train_size, val_test_size],\n",
    "    generator=Generator().manual_seed(10)\n",
    ")\n",
    "\n",
    "val_size = val_test_size//2\n",
    "test_size = len(val_test_dataset) - val_size\n",
    "\n",
    "val_dataset, test_dataset = random_split(\n",
    "    val_test_dataset, \n",
    "    [val_size, test_size],\n",
    "    generator=Generator().manual_seed(10)\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain size: {len(train_dataset)}\")\n",
    "print(f\"Val size: {len(val_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# for batch_images, batch_labels in train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0]\n",
    "img = img.numpy().transpose(1, 2, 0)\n",
    "label = train_labels[0]\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_transformed_images(image_paths: list, transform, num_samples=3, seed=10):\n",
    "    random.seed(seed)\n",
    "    random_image_paths = random.sample(image_paths, k=num_samples)\n",
    "    for image_path in random_image_paths:\n",
    "        with Image.open(image_path) as f:\n",
    "            fig, ax = plt.subplots(1, 2)\n",
    "            ax[0].imshow(f) \n",
    "            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n",
    "            ax[0].axis(\"off\")\n",
    "\n",
    "            # Transform and plot image\n",
    "            # Note: permute() will change shape of image to suit matplotlib \n",
    "            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n",
    "            transformed_image = transform(f).permute(1, 2, 0) \n",
    "            ax[1].imshow(transformed_image) \n",
    "            ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")\n",
    "            ax[1].axis(\"off\")\n",
    "\n",
    "            fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n",
    "\n",
    "folder_path = data_root_dir / \"ffhq_real_faces\"\n",
    "image_names = os.listdir(folder_path)\n",
    "image_paths = [folder_path / image_name for image_name in image_names]\n",
    "\n",
    "view_transformed_images(image_paths, image_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "- apply more transforms; also, do we need to normalise after converting to tensor?\n",
    "- dealing with class imbalance\n",
    "- K-fold cross-validation? Since we don't have a huge amount of data\n",
    "- performance metrics\n",
    "- batch size (depends on GOU memory capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
